{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Introduction to Dataframes\n",
    "\n",
    "Author: Laura Gutierrez Funderburk\n",
    "\n",
    "Created on: April 21 2018\n",
    "\n",
    "Last modified on: April 21 2018\n",
    " \n",
    "### Abstract\n",
    "\n",
    "In this notebook I will introduce dataframes and run through a few examples. Please note this only an introductory exercise. Participants are welome to learn more about it. \n",
    "\n",
    "Here are a few resources:\n",
    "\n",
    "<a href=\"https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python\" target='_Blank'>DataCamp.com</a>\n",
    "\n",
    "<a href=\"https://pandas.pydata.org/pandas-docs/stable/dsintro.html\" target=\"_blank\">PyData.org</a>\n",
    "\n",
    "My goal in this exercise is to provide the workshop participants with a taste of what dataframes can do and to provide an opportunity to explore and learn outside this workshop. \n",
    "\n",
    "### About Python dataframes\n",
    "\n",
    "Dataframes can be called via the Pandas library. Python dataframes are two-dimensional labelled data structures whose columns may or may not contain different data types. Columns and rows are indexed and can be labelled. \n",
    "\n",
    "This is like dialling up what we have been learning so far by using comprehension lists and dictionaries, as dataframes can be thought of as dictionary-based <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html\" target=\"_blank\">numpy arrays</a> $^{[1]}$.\n",
    "\n",
    "$^{[1]}$ A numpy array is an array that can hold different data types: float, int, str, arrays,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm up Examples\n",
    "\n",
    "We will start by taking small arrays and defining dataframes. We will use pandas function read_csv to read from out data.csv file and, with the help of a few functions I have defined, I will showcase how powerful they can be in conjunction with dictionaries and comprehension lists when dealing with output that looks nothing like a table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We begin by importing pandas library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let us take our celestials example along with the individuals we sent out into space\n",
    "celestials = ['Moon','Sun','Neptune','Mars','Jupiter','Venus']\n",
    "# Define space_travellers where each entry is a person\n",
    "space_travellers = [\"James\",\"Sonia\",\"Vero\",\"Tom\",\"Lily\",\"Manny\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataframes via help of our dictionary notation\n",
    "space_df = pd.DataFrame({\"Celestial Bodies\": celestials, \"Space Travellers\": space_travellers})\n",
    "print(space_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the pandas library to read csv files and extract information as we need. For instance, by printing the data below we no longer need to refer to the original file!\n",
    "\n",
    "We can also isolate specific columns within our file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./DATA/data.csv\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print specific columns. \n",
    "print(data.Cluster_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes and plotting our data\n",
    "\n",
    "In the analyze.py script I have predefined a number of functions that deal with our output-.files. As the participants can notice, these are messy files, but with the help of comprehension lists, we can massage and transform into arrays we can later manipulate using dataframes. \n",
    "\n",
    "Below I will define a few functions and showcase how the use of dataframes made the interpretation of output results much easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run analyze.py script\n",
    "%run -i analyze.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate where to find all files of the form output-*.out\n",
    "output = \"./DATA/OUTPUT_FOR_PLOTTING/\"\n",
    "# Use glob library to store file names in array\n",
    "all_the_files = glob.glob(output + \"output-*.out\")\n",
    "print(all_the_files[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We define a function that takes as input a file array, as above and outputs a \"dirty\" dataframe\n",
    "\n",
    "def data_to_dataframe(file_array):\n",
    "    \n",
    "    \"\"\"This function will turn all data stored in tables for F1,F2 into dataframes\"\"\"\n",
    "    \n",
    "    # Store indeces for files which are free from error\n",
    "    file_index = select_index(file_array)\n",
    "    \n",
    "    # Turn data in F1 into dataframe, attach column names\n",
    "    F1_df = pd.DataFrame([extract_family_length_scores(file_array[i],1) for i in file_index],\\\n",
    "                      columns = [\"Family\", \"Number_Seq\", \"Q_Score\", \"TC_Score\",\"Cline_Score\"])\n",
    "    \n",
    "    # Turn data in F2 into dataframe, attach column names\n",
    "    F2_df = pd.DataFrame([extract_family_length_scores(file_array[i],2) for i in file_index],\\\n",
    "                      columns = [\"Family\", \"Number_Seq\", \"Q_Score\", \"TC_Score\",\"Cline_Score\"])\n",
    "    \n",
    "    # Return both datafrains as a 2-tuple\n",
    "    return (F1_df,F2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data_frame(data_frame_pair):\n",
    "    \n",
    "    \"\"\"This function cleans our dataframes\"\"\"\n",
    "    # empty array: save cleaned dataframes\n",
    "    clean_family_dataframes = []\n",
    "    \n",
    "    # Loop through both family dataframes\n",
    "    for i in range(2):\n",
    "        # dataframe on variable clean_Fi_df\n",
    "        clean_Fi_df = data_frame_pair[i]\n",
    "        # remove '/n' from columns Cline_Score and Family\n",
    "        clean_Fi_df['Cline_Score'] = clean_Fi_df['Cline_Score'].map(lambda x: x.rstrip('\\n'))\n",
    "        clean_Fi_df['Family'] = clean_Fi_df['Family'].map(lambda x: x.rstrip('\\n'))\n",
    "        # Turn Cline, Q and TC scores into float (originally they are coded as strings)\n",
    "        clean_Fi_df['Cline_Score'] = clean_Fi_df['Cline_Score'].apply(lambda x:float(x))\n",
    "        clean_Fi_df['Q_Score'] = clean_Fi_df['Q_Score'].apply(lambda x:float(x))\n",
    "        clean_Fi_df['TC_Score'] = clean_Fi_df['TC_Score'].apply(lambda x:float(x))\n",
    "        clean_Fi_df['Number_Seq'] = clean_Fi_df['Number_Seq'].apply(lambda x:int(x))\n",
    "        # store clean_Fi_df into array\n",
    "        clean_family_dataframes.append(clean_Fi_df)\n",
    "        \n",
    "    # return array with clean versions of F1, F2\n",
    "    return clean_family_dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply data_to_dataframe on all output files\n",
    "data_pair = data_to_dataframe(all_the_files)\n",
    "\n",
    "# Cleaning data files\n",
    "clean_data_pair = clean_data_frame(data_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframes for both clusters\n",
    "F1_Data_Frame = clean_data_pair[0]\n",
    "F2_Data_Frame = clean_data_pair[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(F1_Data_Frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frequency(F2_Data_Frame,\"F2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_number_seq_vs_scores(F2_Data_Frame,\"F2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn\n",
    "\n",
    "Open the analyze.py script along with the output-.out files and discuss with a peer how the use of comprehension lists and dataframes made plotting results possible. \n",
    "\n",
    "What other kinds of information can you extract from the F1_Data_Frame and the F2_Data_Frame?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
